---
layout: post
title: Graph Attention Networks
tags: [graph]
---

# 소개

Graph Attention Network (GAT)는 그래프 데이터에 masked self-attention의 이점을 적용하여 기존 graph convolution의 단점을 보완할 수 있는 모델이라고 소개하고 있다. 이 논문은 transformer의 self-attention 구조에서 영감을 얻어서 다음과 같이 node-classification에 적용시키고 있다. 각 노드에 대한 hidden representation을 계산할 때, 이웃을 방문하는 개념을 self-attention을 통해서 적용하는 것이다.

Attention 구조는 다음과 같은 특징들이 있다.

1. 노드-이웃 쌍을 평행하게 계산할 수 있기 때문에, 효율적이다.
2. 서로 다른 차원의 그래프 노드에도 이웃들에 임의의 weight를 정의하여 적용할 수 있다.
3. 모델을 미확인된 그래프에 일반화시키는 등의 inductive 문제에도 바로 적용이 가능하다.

이 논문에서는 Cora, Citeseer, Pubmed citation network, inductive protein-protein interaction 데이터셋의 총 4개의 벤치마크에 대해서 측정을 하고, SOTA급의 수준을 달성했다.

# Graph Attentional Layer

GAT 모델의 기초가 되는 단일 graph attentional layer에 대해 설명하도록 한다. 먼저, 여기서 사용되는 기호에 대해 아래와 같이 정의한다.

- 레이어의 입력은 node feature의 집합이다. 이를 $h=\{\vec{h_1},...,\vec{h_N}\}, \vec{h_i}\in \mathbb R^F$로 표기한다. $N$은 노드의 개수를, $F$는 각 노드의 feature 개수를 나타낸다.
- 레이어의 출력은 입력과 다른 새로운 node feature의 집합이다. 이를 $h'=\{\vec{h'_1},...,\vec{h'_N}\},\vec{h'_i}\in\mathbb R^{F'}$로 표기한다. $F'$는 출력 노드의 node feature 개수이다.

- 입력 feature 들을 고수준의 feature 로 변환하기 위해서는 최소한 한개의 선형 변환이 있어야 한다. 이 선형변환의 weight를 $W\in\mathbb R^{F'\times F}$로 표기한다.

### Self-Attention

위의 선형변환 후에는 self-attention 단계를 수행하게 된다. 이 self-attention 변환을 $a:\mathbb R^{F'}\times\mathbb R^{F'}\rarr\mathbb R$라고 할 때, 이 변환에 사용되는 내용들은 아래와 같다.

#### Attention Coefficients (attention 계수)

각 attention coefficient 값은 다음과 같이 표현할 수 있다.

$$
e_{ij}=a(W\vec{h_i},W\vec{h_j})
$$

여기서 attention 계수 $e_{ij}$는 노드 $j$의 각 feature들이 노드 $i$에 얼마나 중요한지에 대한 값이다.

#### Masked Attention

Masked attention이란, $e_{ij}$를 계산할 때, $j$가 $i$의 이웃 노드일 경우에 대해서만 계산을 수행하는 작업을 말한다.

위 식은 그래프의 구조에 대한 정보를 뺐기 때문에 모든 노드와 노드끼리의 방문이 허용되는 구조이다. 그래서 여기서 그래프의 구조 정보를 주입하기 위해 masked attention을 수행한다. 논문의 모든 실험에서 이들이 정확히 $i$의 1차 이웃(자기자신 포함)이 된다고 한다.

#### Normalized Attention Coefficients

이 계수들을 다른 노드들과 쉽게 비교하기 위해 다음과 같이 softmax를 통해서 normalize시키면 다음과 같은 식이 된다.

$$
\alpha_{ij}=softmax_j(e_{ij})=exp(e_{ij})/\sum_{k\in N_i}exp(e_{ik})
$$

이 실험 결과에서 attention 매커니즘 $a$는 weight 벡터 $\vec a\in \mathbb R^{2F'}$ 단일 레이어 feedforward 네트워크를 나타낸다. 그리고 여기에 $LeakyReLU$를 적용한다. 따라서 최종적인 attention 결과는 다음과 같이 된다.

$$
\alpha_{ij}=\frac{LeakyReLU(\vec a^T[W\vec h_i\Vert W\vec h_j])}{\sum_kexp(LeakyReLU(\vec a^T[W\vec h_i\Vert W\vec h_k]))}
$$

여기서 $\Vert$는 concat을 나타낸다.

#### 최종 출력

이렇게 계산된 노드들의 normalized attention coefficients를 선형변환을 통해 모든 노드들의 최종 출력 feature를 구하게 된다. 각 레이어 출력의 식은 다음과 같다.

$$
\vec {h'}_i=\sigma(\sum_{j\in N_i}\alpha_{ij}W\vec h_j)
$$

여기서 self-attention 과정의 안정성을 위해서 transformer논문에서와 마찬가지로 multi-head attention을 적용한다. $K$개의 헤드를 적용한다고 하면, 식은 다음과 같다.

$$
\vec {h'}_i=\underset {k=1}{\overset K\Vert}\sigma(\sum_{j\in N_i}\alpha_{ij}^kW^k\vec h_j)
$$

마지막 multi-head attention 레이어에는 concat이 필요없다. 따라서 concat 대신에 다음과 같이 전체 head들의 attention에 대한 평균을 구한다.

$$
\vec {h'}_i=\sigma(1/K\sum_{k=1}^K\sum_{j\in N_i}\alpha_{ij}^kW^k\vec h_j)
$$

이런식으로 각 노드에 대한 모든 attention 레이어를 구성할 수 있고, 이 것이 바로 GAT가 된다.

# 결론

논문에는 GAT 성능에 대한 다양한 비교와 실험 결과가 있는데, 이 부분은 생략하도록 한다.

GAT를 통해 그래프 문제를 attention 으로 풀기 위한 기반이 되었다.

GAT의 가장 큰 장점중 하나는 바로 전체 그래프구조가 필요하지 않다는 것이다. 기존의 [GCN](/gcn)만 보더라도, 전체 그래프의 인접행렬 $A$를 통해서 normalized laplacian matrix를 구해야 했고, 계산비용뿐만 아니라 그래프의 변화에도 취약했다.

하지만 GAT는 해당 노드의 인접노드 정보만으로도 feature 를 구할 수 있으며, 그 성능이 좋기 때문에 transductive뿐만 아니라 inductive 문제, 특히 예를들면 완전히 미확인된 그래프를 테스트셋으로 활용하는 등이 가능하게 되었다.
