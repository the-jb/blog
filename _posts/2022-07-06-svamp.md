---
layout: post
title: "Are NLP Models really able to Solve Simple Math Word Problems?"
tags: [논문, NLP, MWP]
---


Math Word Problem (MWP) 는 아직까지 많은 관심을 받고 있는 분야는 아니다. 하지만 최근 들어서 이 분야에 대한 연구들이 많이 늘어나고 있다. 구글에서도 최근에 [Minerva](https://arxiv.org/abs/2206.14858) 라는 PLM 모델과 논문을 내는 것을 보았을 때, 이런 수학적 추론분야에 대해 관심을 갖기 시작한 것으로 보인다. 이 논문은 개인적으로 MWP에 대해 관심을 갖게 만든 논문이었기에, MWP 분야에 대한 내용과 함께 소개하고자 한다.

- 논문 : [arXiv](https://arxiv.org/abs/2103.07191)

# Math Word Problem (MWP)

MWP는 우리나라식 표현으로 "수학 문장제 문제" 라고 이해하면 된다. 그리고 보통 MWP 태스크라고 하면, 수학 문장제 문제를 풀어서 답을 맞추는 것을 의미한다. 객관식 태스크도 있지만, 보통 주관식 풀이에 대해 연구가 많이 되고 있고, 개인적으로도 수학을 푸는데 객관식은 의미가 없다고 생각한다. 데이터셋은 아주 쉬운 문제부터 어려운 문제까지, 식을 맞추는 데이터셋에서 풀이과정을 설명하는 데이터셋까지 다양한 방식으로 존재한다. 즉, 아직까지 어떤 방향이 좋을 지 정형화 되어있지가 않은 태스크라는 뜻이다. 또한, 중국에서 이 분야에 대해 많이 연구해서 중국어 데이터셋들도 많이 있다.

여기에서 다루는 MWP 는 MWP중에서도 가장 쉬운 난이도, 아주 간단한 사칙연산 문제를, 주관식으로 식을 세워서 푸는 문제들만으로 한정한다. 모델은 문제를 보고 그에 맞는 식을 세워서 정답을 출력한다.[^1] 여기에 해당하는 영어 데이터셋은 MAWPS, ASDiv-A[^2] 가 있다.

[^1]: 보통 식이 맞았는지는 여부보다는, 정답 여부로만 성능을 평가한다.
[^2]: ASDiv 데이터셋의 Arithmetic subset

# NLP 모델이 정말 간단한 수학 문제를 풀 수 있을까?

논문의 제목처럼 이 논문은 어떤 모델을 제안하는 논문이 아니라 현재 NLP 모델에 대한 실험과 새로운 데이터셋(SVAMP)를 제시하는 논문이다. Simple MWP 문제들은 이미 기존 모델들이 좋은 성능을 달성했다. 그래서 이제 조금 더 복잡한 문제까지 풀기 위한 데이터셋과 논문들이 나오고 있었다. 하지만 이 논문은 현재 NLP 모델들이 달성한 수치에 대해 아래 실험들을 통해 shallow heuristic 문제를 제기한다.

실험의 베이스라인 모델은 기존 MWP 에서 sota 성능을 냈었던 GTS, Graph2Tree 와 기본적인 Seq2Seq (LSTM) 모델에 RoBERTa를 적용해서 사용하였다.

## Question-removed MWPs

이 논문에서 현재 모델들이 제대로 MWP를 풀지 못한다는 것을 보여주는 첫 번째 실험이다. 여기에서 질문을 삭제했다는 것은, 문제에서 조건들만 남겨두고 테스트를 했다는 것을 의미한다. 예를 들면 다음과 같다.

- 원래 문제
> Jack had 8 pens and Mary had 5 pens. Jack gave 3 pens to Mary. **How many pens does Jack have now?**
> 정답 : $8-3$
- 질문 삭제
> Jack had 8 pens and Mary had 5 pens. Jack gave 3 pens to Mary. ~~**How many pens does Jack have now?**~~
> 정답 : 알 수 없음

원래 문제는 Jack 이 현재 몇 개의 pen을 가지고 있는지를 계산하기 위해 $8-3$ 이라는 식을 세웠다. 하지만, 질문을 삭제한다면, 이 문제는 아무것도 질문하고 있지 않기 때문에, 식을 세울 수가 없다. 따라서, 모델들이 정말로 수학을 이해하고 푼다면 이 테스트셋들을 맞추지 못해야 정상이다. 하지만, 이 실험의 결과는 아래와 같다.

| 모델 | MAWPS | ASDiv-A |
| ----| ------| --------|
| Seq2Seq | 86.7 -> 77.4 | 76.9 -> 58.7 |
| GTS | 88.5 -> 76.2 | 81.2 -> 60.7 |
| Graph2Tree | 88.7-> 77.7 | 82.2 -> 64.4 |


정확도가 약 10%정도 떨어졌지만, 그래도 모델들이 질문도 안한 문제들의 정답을 꽤 높은 성능으로 맞추고 있다. 결국 모델들이 제대로 문제를 보지 않고 식을 세운 것이 아닐까? 의심이 가는 것이다. 이 것이 이 논문에서 주로 지적하는 문제인 shallow heuristics 문제이다. 즉, 모델이 편법을 사용해서 문제를 해결한다는 것이다.

물론 아직까지는 이 실험결과로 모델들이 편법만 쓰고 있다고 말할 수는 없다. 편법을 잘한다고 수학을 못한다고 말할 수는 없기 때문이다.

## Shallow Heuristics

Shallow heuristics 는 쉽게 말하면 일종의 편법 정도로 생각하면 될 것이다. 최근 들어 여러 NLP 태스크에서 이 shallow heuristics 문제를 얘기하고 있고, MWP 가 이러한 문제를 정말 직관적으로 보여주는 좋은 태스크라고 생각한다.

이 논문에서는 데이터셋에 어떤 shallow heuristics 이 사용되었는지를 보여주기 위해 seq2seq 를 변형시킨 모델을 만들었다. seq2seq에서 디코더는 그대로 두고, 인코더 대신에 `feed forward network`를 넣는다. 쉽게 얘기하면, 문장을 읽는 것이 아니라, 그냥 순서없는 단어들을 보고 문제를 푼다는 것이다.

수학 문제 자체를 이해할 수 없기 때문에, 문제를 풀지 못해야 한다. 하지만 이 모델에서도 성능이 MAWPS는 77.9%나 나온다. 어떻게 이런 성능을 달성하는지 분석하기 위해 논문에서는 이 모델의 attention weight 을 분석하였고, 특정 단어에만 가중치가 쏠려있는 것이 확인되었다.

예를 들면 다음과 같다.

> John delivered 3 letters at **every** house. If he delivered for 8 houses, how many letters did John deliver?
> 정답 : $3\times 8$

every 등의 단어는 보통 "모든 ~ 에" 라는 뜻으로 쓰이기 때문에, 문제가 곱셈일 확률이 높을 수밖에 없다. 따라서 이 단어를 보면 그냥 문제에 나와있는 두 숫자를 곱하기만 해도 웬만하면 정답이 된다는 얘기가 된다.

이 실험결과로, 논문에서는 기존 데이터셋이 이렇게 특정 단어에 의존한 shallow heuristics에 굉장히 취약하게 만들어졌다는 것을 보여준다. 기존 데이터셋은 일반적으로 사람의 입장에서 푸는 수학문제들을 만들다보니, 모델이 이런 식으로 편법을 사용할 지를 몰랐던 것이다.

그러면 모델이 편법을 쓰지 않고 문제를 풀면 어느정도 성능이 나올까? 이에 대한 대답이 바로 다음 장에 제시된다.

# SVAMP

Simple Variations on Arithmetic Math word Problems (SVAMP)는 이 논문의 main contribution 인 데이터셋이다. 이 데이터셋은 ASDiv-A 데이터셋의 일부 데이터셋을 변형시켜서 만들었다. 변형 종류는 다음과 같다.

- **Question Sensitivity** : 문제 조건을 유지하고 질문을 미묘하게 변형시키는 것이다. 답을 다르게 만들거나, 같은 답을 다르게 질문하는 등을 수행한다.
  > How many pens does **Jack** have now?
    -> How many pens does **Mary** have now?
- **Reasoning Ability** : 문제의 조건을 변형시켜서 답을 다르게 만드는 것이다.
  > **Jack** gave 3 pens to **Mary**.
    -> **Mary** gave 3 pens to **Jack**.
- **Structural Invariance** : 같은 문장을 문법적으로 변형시키거나, 쓸데없는 정보를 추가하는 것이다.
  > Jack had 8 pens and Mary had 5 pens.
    -> Jack had 8 pens, **Tom had 12 pens** and Mary had 5 pens.


실험 방식은 MAWPS 와 ASDiv-A로 학습시킨 뒤, SVAMP로 정확도를 계산하는 방식이다. SVAMP 자체가 ASDiv-A 를 기반으로 하기 때문에, 테스트셋으로 활용하기 문제 없다고 보면 된다. 그리고 테스트셋의 난이도 자체도 ASDiv-A 와 거의 동일하다고 할 수 있다. 하지만, 실험 결과는 그렇지 않다.

| 모델 | MAWPS | ASDiv-A | SVAMP |
| --- | ----- |------| ------- |
| Seq2Seq | 86.7 | 76.9 | 40.3 |
| GTS | 88.5 | 81.2 | 41.0 |
| Graph2Tree | 88.7 | 82.2 | 43.8 |

SOTA 모델인 Graph2Tree 가 기존 데이터셋들에 대해서는 80% 이상의 성능을 보였던 반면에, SVAMP는 절반도 맞추지 못하고 있다. 대부분 문제들이 연산자 1개만 들어가서 식을 세우는 경우의 수가 얼마 되지 않는 것을 고려해보면 처참한 수준이다.

위의 실험에서는 모델에 shallow heuristics 가 많이 포함되는 것을 보였다면, 이 실험에서는 모델이 편법을 쓰지 못할 경우에 푸는 능력이 거의 없다는 것을 보여주고 있다. 즉, 현재 모델들은 shallow heuristics 에만 의존해서 SOTA 성능을 내고 있었다는 것을 알 수 있다.


# 결론

최근에 AI는 폭발적인 발전을 이루고 있고, 엄청난 결과들을 보여주고 있다. 수많은 기업들과 스타트업에서 다양한 AI들을 개발하고 서비스들을 출시한다. 이제는 AI가 사람을 대체하는 것이 아니냐는 얘기가 정말 많이 나오고 있다.

하지만 이 논문은, AI가 ***"아직은 멀었다"*** 를 보여주는 재미있는 결과물이었다. 여기에서 나온 문제들은 정말 초등학교 1학년 과정에서 배우는 수준의 문제들이다. 이미 SOTA 모델들이 이런 문제들은 이미 높은 성능을 보여주고 있었기에, 대부분의 연구들은 더 어려운 문제들에 도전하고 있었다. 그러나 여기에서 보여주듯이 생각보다 모델들이 shallow heuristics 에 심각하게 의존하고 있었고, 모델의 생각하는 능력 자체는 초등학생 수준도 되지 못했던 것이다.

개인적으로 이런 면 때문에 MWP 태스크에 많이 흥미를 가지게 되었다. 우리들도 학교에서 수학을 배우면서 논리적인 사고력을 기르듯이, NLP 모델이 정말로 생각하는 능력을 갖추려면 이런 수학문제를 푸는 능력이 필요하다고 생각한다.

아직 MWP 는 다른 태스크에 비해 정말 마이너하다. 최근들어 논문들이 많이 나오긴 하지만, 대부분 중국에서 나오고 있고 그래서 중국어 데이터셋들이 영어 데이터셋보다 더 많이 쓰이고 있다. 반면에 한국에서는 이 MWP에 대해 거의 연구하고 있는 곳이 없는 것으로 알고 있다. 하지만 이 MWP 태스크는 생각보다 AI의 발전에 있어서 중요한 역할을 한다고 생각하고, 우리 나라에서도 이 태스크에 대해 좀더 활발하게 연구가 되었으면 하는 바람과 함께 이 포스트를 마친다.
