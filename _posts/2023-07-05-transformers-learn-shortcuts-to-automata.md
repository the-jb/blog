---
layout: post
title: "Transformers Learn Shortcuts to Automata"
tags: [논문, Transformer, Automata]
---

이번 ICLR 2023 에서 정말 멋진 논문을 오랜만에 봐서 이에 대해 포스팅을 남겨보려고 한다.
Transformer 의 동작 방식을 훌륭한 intuition으로부터 증명까지 하고, 이에 대해서 실험적으로도 충분하게 입증한다.
다만, 논문이 상당히 어려운 편이다.
특히 뒷부분 정리의 증명 과정은 사실상 계산이론의 탈을 쓴 추상대수학으로, [Krohn-Rhodes 정리](https://doi.org/10.2307%2F1994127)로부터 세미오토마타를 여러 모듈로 분해하는것부터 출발하는데 이 과정에만 Appendix 30페이지를 넘게 사용한, 거의 이론 저널에 가까운 컨퍼런스 논문이다.

하지만, 이 논문은 그러한 디테일 없이도 정말 유용한 관점들이 많이 담겨있다.
이 포스팅에서는 추상대수학이나 계산이론에 대한 깊은 지식이 없는 사람들도 증명의 결과물들과 이 것이 어떻게 Transformer를 설명하는지 까지를 이해하기 쉽도록 최대한 자세히 풀어보고자 한다.


- 논문 : [OpenReview](https://openreview.net/forum?id=De4FYqjFueZ), [arXiv](https://arxiv.org/abs/2210.10749)


# 오토마타와 뉴럴 아키텍처

먼저, 제목을 보면 *왜 Transformer 에 오토마타를 학습시켜야 하는가?*, *뉴럴 네트워크에 왜 오토마타가 필요한가?* 이런 질문들부터 떠오를 것이다.
여기서 오토마타는 **우리의 추론 과정을 아주 추상화시킨 형태** 로 이해해야 한다.
즉, 이 논문에서는 Transformer 가 어떻게 추론을 수행하는지, 그리고 아키텍처적으로 근본적인 한계점이 무엇인지까지를 매우 수학적인 형태로 다루고자 하는 것이다.

이 논문에서 다루는 것은 세미오토마타(Semiautomata)로, 일종의 DFA (가장 기본적인 형태의 오토마타) 라고 볼 수 있다.[^1]
그래서 논문을 이해하기 위해서는 가장 먼저 세미오토마타가 무엇인지를 파악해야 한다.

[^1]: 이보다 확장된 오토마타(푸쉬다운 혹은 CFG 등)까지는 다루지 않는다.

## Semiautomata

![Semiautomata 예제(논문 출처)](.images/transformers-learn-shortcuts-to-automata/semiautomata.svg)

딥러닝에 관심이 있는 사람들이면, 오토마타의 개념보다는 Markov Process가 더 친숙할 것이다.
오토마타는 Markov Process 처럼 어떤 상태가 다른 상태로 전이하는 머신이라고 생각하면 된다.
위 그림과 같이 어떤 상태(state)에서 입력(input)에 따라 다른 상태로 전이하는 과정을 표현한 모델을 세미오토마톤(단수형)이라고 한다.
(왜 semi 라는 표현이 붙었냐면, final state 개념이 없기 때문인데, 이 논문에서는 중요한 부분이 아니니 넘어가도록 한다.)
여기에서 $Q$는 상태의 집합을 말하고, $\Sigma$는 입력의 종류의 집합이다.

위 그림에서 눈여겨봐야 할 부분은 memory unit 부분인데, 논문의 뒷부분의 핵심 증명이 되는 단위이다.
이 메모리 유닛은 회로에서는 Flip-Flop 이라고도 불리는, 1비트를 저장하는 장치를 오토마타로 표현한 것이다.
위 그림에서는 클로버나 다이아 모양을 메모리에서 읽고($\perp$) 쓰는($\sigma$) 작업을 표현하고 있다.

## 뉴럴 아키텍처

NLP 의 대표적인 아키텍처 RNN과 Transformer는 꽤나 다른 특성을 가지고 있다.
RNN은 거의 일종의 오토마타같이 어떤 state에서 입력을 받으면, 다른 state로 전환하는 재귀 형태의 구조이다.
따라서, 단순하게만 생각해도 위와 같은 세미오토마타를 거의 완벽하게 표현할 수 있다.
여기서 표현한다는 개념은 쉽게 얘기해서 어떤 순차적인 입력들이 들어올 때, 최종적으로 어떤 상태에 도달하는지를 완벽하게 파악하는 모델을 만들 수 있냐는 것이다.

하지만 Transformer는 재귀적인 구조가 아니다.
한 레이어에서 다른 레이어로 순차적으로 전파되기 때문에, 결국 가장 단순하게 생각하면 레이어 개수만큼만 전이가 가능하다라고도 볼 수 있다.
그런데 실제로 Transformer는 레이어 개수에 비해서 훨씬 많은 길이의 문장들을 상당히 잘 처리한다.

이 처리하는 비결이 바로 shortcut을 학습하기 때문이라는 것이 논문 내용의 핵심이다.
그렇다면 도대체 shortcut이란 무엇이고, 이런 결론이 도대체 어떤 의미가 있는지를 살펴볼 것이다.

# Shortcut

딥러닝을 연구하는 사람들이면 이 shortcut이라는 단어는 매우 친숙하게 보일 것이다.
가장 최근의 LLM 이전까지 전 분야에 걸쳐서 꽤나 핫하게 연구되고, 많은 의문을 제기한 단어이기 때문이다.

바로 [이전 포스팅](/svamp)에서 다루었던 내용도 일종의 shortcut이라고 볼 수 있다.
수학 공식을 정확히 이해했다면, 다른 방식으로 문제를 만들어도 풀 수 있지만, shortcut만을 배운다면, 문제를 좀 변형해서 냈을 때 틀린 답을 도출하게 되는 것이다.

shortcut이란 이와 같이, 어떤 문제를 해결하는 정해가 아닌 *쉬운 길*, 즉 지름길을 뜻한다.
결국, 그 풀이 방식으로는 완벽히 문제를 풀 수 없으며, 어떤 예외적인 경우들이 존재하게 되는 의미를 갖고 있기도 하다.

Transformer가 shortcut을 배운다는 것을 증명하려면, 먼저 shortcut이 무엇인지 정확한 정의가 필요하다.
문제를 푸는 정해가 아니라는 것은 이해하기 쉽다.
하지만 어떤게 *쉬운 길*인가? 이를 어떻게 표현해야 할까?

> 이 논문에서 오토마타를 사용한 이유는 바로 이런 *길*을 추상화하기 위해서라고 생각한다.
우리가 문제를 추론하는 단계를 오토마타로 표현한다면, 이 오토마타로부터 나타나는 상태변화의 나열하다보면 바로 *길*이 되는 것이다.

그렇다면 다시, shortcut이란 무엇일까?
오토마타로 추상화를 했기에 이제는 쉽게 대답할 수 있다.
바로 원래 정답을 찾기 위해 필요한 상태변화의 길이보다 짧은 길로 정답에 도달하면, shortcut이 되는 것이다.
이를 수학적으로 표현하면 다음과 같다.

> **Definition 1)** *어떤 세미오토마톤 $\mathcal A$에 대해서 $D$깊이의 모델이, 그 깊이보다 더 긴 $T$ 길이 (즉, $o(T)\geq D$) 의 sequence까지 정답을 맞춘다면, 그 모델이 문제를 푸는 방법이 바로 $\mathcal A$에 대한 **shortcut solution**이다.*

정말 직관적이고 유용한 정의가 되었다.
이 논문이 흥미로웠던 이유는 이렇게 실제의 문제들을 정말 직관적으로 와닿게 잘 추상화하고, 그로부터 정말 유의미한 결론들을 얻어냈기 때문이다.

# Transformer Learns Shortcuts

> Transformer는 왜 잘 동작할까? ***Shortcut을 배우기 때문이다.***

이 논문은 이러한 결론을 내기 위해 달리고 있다.
그렇다면 다음과 같은 궁금증이 생길 것이다. Transformer는 (1) 도대체 어떻게 shortcut을 배우고 (2) 그렇게 배운 shortcut은 얼마나 효과적이길래 잘 작동하는 것일까?

먼저 (2)에 대해 살펴보도록 한다.
[Transformer 논문](/attention-is-all-you-need)에서 사용한 레이어어는 깊이 $D=6$을 가진다.
최근의 LLM들은 보통 수십개~100개 까지 레이어를 늘리고 있다.
이런 깊이는 사실 입력 문장의 길이를 생각하면 매우 짧은 편이다.

그렇다면, $D$ 깊이의 레이어는 얼마나 긴 문장까지 소화하는 shortcut을 만들어 낼 수 있을까?
결론부터 말하면 다음과 같다.

> **Theorem 1)** *Transformer 구조는 $O(\log T)$ 만큼의 깊이만 있으면 어떤 세미오토마타든지 $T$ 길이까지 표현할 수 있다. 여기에는 임베딩과 어텐션션의 크기가 상태 개수만큼, $O(\vert Q\vert)$, MLP 크기는 상태 개수의 제곱만큼, $O(\vert Q\vert^2)$ 이 필요하다.*

알고리즘에 익숙한 사람들이라면, $O(\log T)$를 보았을 때 느꼈을 수도 있겠지만, 바로 분할정복 형태의 Transformer 모델을 구성하는 것이다.
아래 그림이 바로 이러한 분할정복 형태의 Transformer를 표현한 것이다.
이제 (1)의 질문에 대답할 수 있다.
각 토큰에마다 모든 상태를 표현할 수 있는 임베딩 크기만 있다면, 입력 크기에 비해 아주 적은 깊이로도 shortcut 동작 모델을 만들어낸 것이다.

![Theorem 1 Intuition (논문 출처)](.images/transformers-learn-shortcuts-to-automata/theorem1.svg)


개인적으로 뒤의 훨씬 복잡한 증명보다, 이 정리가 논문에서 얻을수 있는 가장 유용한 직관이 아닐까 생각한다.
이 정리의 핵심은 세미오토마톤을 입력 sequence 길이 $T$ 까지는 $O(\log T)$ 깊이의 모델로 무조건 shortcut을 만들어낼 수 있다는 것에 있다.

깊이가 깊어질수록 습득할 수 있는 문장 길이가 깊이의 지수배만큼 길어지는 것이다.
그래서 재귀 구조가 아닌데도 사실상 현실의 문장들을 훌륭하게 학습해냈던 것이다.

논점과는 다른 방향이긴 하지만, 여기에서 또 한가지 흥미로웠던 부분은, MLP(=Feed-Forward 레이어)의 크기는 다른 레이어들과 다르게 $O(\vert Q\vert^2)$ 이 필요하다는 것이다.
최근 트랜스포머의 Feed-Forward 레이어들의 역할에 대한 연구들을 재미있게 봤는데, 이러한 연구들과 매우 일맥상통하는 결론을 이론적으로 도출하는 점이 정말 흥미로운 포인트였다.

## 나머지 내용에 앞서,

논문의 나머지 정리들은 세미오토마타의 특성을 활용해서 위의 shortcut의 효율을 훨씬 더 개선하는 증명들과, 실질적인 실험을 보여주는 부분으로 이루어져 있다.
물론 뒷부분의 정리들은 정말 이론적으로는 멋있는 부분들이 많지만, 개인적으로 결론이 더 와닿는 정리는 위의 **Theorem 1**이 아닌가 생각한다.

실제 우리 언어는 단순히 세미오토마타와 같이 현재상태만을 참조하는 오토마타로는 표현하기 힘들고, 최소한 생성문법에서는 보통 CFG, 즉 푸쉬다운오토마타를 많이 다룬다.
그렇기에 Transformer에 DFA를 가득 채워넣는 것은 모델에 대한 직관적인 이해에 있어서는 잘 와닿지 않는 부분이 있다.

하지만 물론, 뒤의 증명들도 정말 매력있고, 좋은 직관으로부터 결론까지 도출한다. 특히 메모리 유닛이라는 개념을 생각해낸 부분이 정말 대단하다고 느낀다.

어찌되었건, 그래서 아래 내용 이전에 결론을 먼저 소개하고, 이 결론에 대해 어떻게 해석하는지에 대해서 논문에서는 없는, 개인적인 해석들을 많이 덧붙이려고 한다.
단, 이 해석들은 분명 증명이 아니며, 실질적인 결과보다 많은 비약이 있다.

# Shortcut의 의미

먼저 실험부터 간단히 소개하자면, 일단 위와 같은 simulation은 모델 자체의 이론적인 한계치이고,
현실적으로 위와 같은 이론상의 성능까지 학습을 할 수 있을지는 또 다른 문제이다.
그렇다면 정말 실제로도 위와 같은 이론적인 성능을 내도록 모델을 학습할 수 있을까?

결과적으로, **실험적으로도 충분히 근접한 shortcut을 구현해 내는 데 성공한다.**

다시, 이러한 오토마타를 우리가 하는 일반적인 자연어 추론으로 치환하면, 왜 Transformer가 잘 동작하는지, 특히 모델의 깊이를 매우 키운 LLM은 어떻게 그렇게 강력한 성능을 내는지, 하지만 왜 완벽하지는 않은지를 설명해낼 수 있다.

Transformer 자체가 매우 강력한 shortcut 일반화 능력을 가지고 있고, 그 범위는 레이어의 폭과 깊이를 키울수록 엄청나게 확장된다.
하지만, 이러한 LLM들이 언어를 근본적으로 인간과 똑같이 이해하냐? 라고 묻는다면 그렇지 않다.
**이 것은 Transformer 아키텍처의 근본적인 한계이기 때문에, 해결할 수 없는 문제이다.**

그러면 다음과 같은 의문이 들 수 있다.

> 지금보다도 훨씬 깊은 레이어를 쌓으면, 거의 무한한 shortcut을 습득하는데, 그러면 결국 사실상 완벽한 것 아닌가?

절반은 맞고 절반은 틀리다고 대답할 수 있다.
아키텍처의 관점에서는 맞다.
단, 훨씬 더 깊은 레이어를 완벽히 학습시키기 위해서는 그만큼의 데이터도 필요한데, 지구상의 데이터로 과연 해결이 가능할까? 에 대해서는 장담할 수 없기 때문이다.

어찌 되었건, 이러한 이론적인 증명부터 실험 결과까지 한 치 의심할 여지 없는 완성도 높은 멋진 논문이라고 생각하며, 이러한 Transformer 아키텍처에 대한 지금까지 없던 새로운 고민 주제를 던져주기도 하였다.

이제 상당히 난도가 있는 나머지 정리들을 다루어보도록 한다.

# Improving Shortcuts

위 분할정복도 상당히 효과적이지만 아쉬운 점이 있다.
Transformer의 어텐션은 global하게 토큰들이 영향을 줄 수 있는 것이 가장 큰 장점인데, 분할정복 형태는 이런 global한 느낌이 없어지기 때문이다.
논문에서는 이러한 이유로 위의 정리를 개선하고자 하였다.
즉, 

> ***Transformer는 좀 더 강력한 shortcut까지도 배울 수 있다.***

라고 얘기하고 싶은 것이다.

이 개선을 위해 이 논문에서 가장 핵심적인 contribution 이자 쉽게 발상하기 어려운 개념인 memory 모듈이 등장한다.
개인적으로, 어떻게 저자가 이런 메모리 형태를 떠올릴 수 있었는지 나로서는 감도 잡히지 않는다.
하지만 결론적으로, 매우 대단한 intuition이었고, 이러한 가설을 통해 멋지게 증명을 완성하였다.
또한, 이 가설은 단순히 증명뿐만 아니라 실제 Transformer도 이런 형태로서 동작할 수 있겠구나 라는 강력한 직관을 안겨주었다.

### Memory Unit

먼저 위의 그림에서 간단하게 보여주었던 Flip-Flop 메모리에 대해서 간단하게 한번 짚고 넘어가도록 한다.
여러 형태의 Flip-Flop이 있지만 여기서는 크게 중요하지 않고, 1비트의 어떤 정보를 기억하고 있다가 반환하는 오토마타를 만든다는 것이 중요하다.
다음과 같이 동작하는 오토마타를 하나의 memory unit이라고 한다.

**Memory Unit)** *메모리로부터 읽기 입력($\perp$)이 들어오면, 기억하고 있던 값을 반환하고, 쓰기 입력($\sigma_0$ 혹은 $\sigma_1$)이 들어오면, 메모리에 있던 값을 0 혹은 1로 대체한다.*

이러한 오토마타를 병렬로 연결할 수도 있다. 그러면, 여러 비트의 메모리들을 관리하는 모듈이 구성된다.

## Decomposition of Semiautomata

이제 본격적으로 세미오토마톤을 분해하면서 논문의 증명이 진행된다.

