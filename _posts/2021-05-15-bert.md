---
layout: post
title: "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
tags: [논문, NLP, Transformer]
---

# BERT

- 논문 자료 : [arXiv](https://arxiv.org/abs/1810.04805)

[Transformer](attention-is-all-you-need)의 등장으로, NLP 연구에 많은 발전이 있었고, BERT를 통해 이를 더 가속화시키게 되었다. 여기서 **BERT**는 **B**idirectional **E**ncoder **R**epresentations from **T**ransformer의 약자로, Transformer 구조에서 인코더 부분을 활용하여 만든 모델이다.

BERT는 language representations (언어 표현)을 위한 모델이다. 따라서 먼저 이 language representations 가 정확히 무엇인지를 이해해야 한다.

## Language Representations

Language representations 는 말 그대로, 언어를 데이터로 표현하는 것을 뜻한다. 기본적으로 컴퓨터는 문자열을 아스키코드, 혹은 유니코드 등을 통해 숫자로 변환시켜서 받아들인다. 이는 언어를 제외하고도 예를 들어 `aS#$%d_(#apoxnd$f` 처럼 모든 문자열 자체를 표현하는 방식이다.

이러한 방식으로 어떤 언어로 이루어진 문장을 표현하면, 그 문장의 뜻에 관해서 컴퓨터가 쉽게 처리하기 어렵다. 예를 들어서 숫자가 높고 낮다고 좋고 나쁜 의미가 되는것도 아니고, 숫자가 가깝다고 뜻이 비슷한 것도 아니다. 이 뜻을 해석하기 위해서는 매우 복잡한 로직을 사용해야 하는 것이다.

하지만, 예를 들어서 시험 점수를 숫자로 표현한다면 이는 컴퓨터가 처리하기 쉽다. 왜냐하면 숫자 자체에서 그 의미를 찾기가 쉽기 때문이다. 숫자가 높은 수는 높은 점수를 나타내고, 낮은 수는 낮은 점수를 나타낸다. 따라서 이런 데이터들은 컴퓨터로 처리하는 로직을 만들기가 쉽다.
따라서 이 시험점수처럼 언어 데이터를 좀 더 의미있는 숫자나 숫자들의 집합으로 표현한다면, 이를 처리하는 로직을 만드는 것이 훨씬 쉬워지겠구나, 라고 쉽게 떠올릴 수 있을 것이다.

여기서 컴퓨터를 딥러닝으로 바꿔도 마찬가지 개념이 된다.

딥러닝에서도 처리하기 쉬운 입력들이 있고, 처리하기 어려운 입력들이 있다. 언어 데이터를 그냥 아스키코드나 유니코드로 입력을 준다면, 모델에서 그 것을 처리하기 굉장히 까다로울 것이다.

따라서 language representations는 언어 데이터를 딥러닝 모델의 입력값으로 쓸 수 있도록 하는 표현을 의미한다. 이 표현이 직관적이고, 포괄적일 수록 간단한 모델도 좋은 결과를 갖게 되는 것이다.

Transformer도 그렇고, 이전의 NLP 모델들도 다 이 language representation 개념을 기본적으로 포함하고 있다. 하지만 이렇게 BERT 에서 이 개념을 설명하는 이유는, 바로 BERT 모델은 language representation 그 자체에 목적을 두고 있기 때문이다. 이후에 어떤 태스크나 모델과도 결합하기 좋은 representation 을 만드는 것이 목적이다.

## BERT 등장의 의의

BERT의 등장에서 가장 중요한 부분은, NLP 모델에서 Pre-training 을 통해 다양한 문제 해결에 좋은 결과를 냈다는 것이다. 이는 다시 말해서, 대부분의 NLP 문제를 풀 때, 꼭 그 문제에 맞는 데이터로 훈련시키는 것이 아니라 일반적인 NLP 데이터로 사전학습을 시키고 이를 모델에 적용해도 좋은 결과를 낼 수 있다는 뜻이 된다.

BERT의 등장으로, 많은 논문들이 이 Pre-trained 모델을 활용해서 다양한 문제를 풀어냈다. 또한, 문제에 맞는 많은 데이터를 수집하기가 쉽지 않은데 BERT가 이런 데이터 부족 문제를 어느정도 해결해 줄 수 있다.

NLP 분야가 지난 몇년간 정말 빠른 속도로 발전하고 있는데, 대부분이 이 BERT를 중심으로 발전하고 있어서 최신의 NLP분야를 이해하는 데 가장 필수적인 논문이라고 할 수 있다.

## 포스팅 소개

이 포스팅에서는 구현물 없이 BERT 모델에 대해 이해할 수 있도록 논문으로부터 내용정리와 추가 설명만 진행한다. BERT는 어차피 기존 Pre-trained 모델을 잘 활용하는 것이 중요하고, 직접적인 구현이 필요한 경우는 없기 때문이다. 이후 BERT에 기반하거나 이를 활용한 논문들을 다룰 때, 해당 구현을 같이 하도록 한다.

# 1. 소개

NLP에서 Pre-training이 효율적이라는 논문들이 많이 등장하였다. 여기에는 문장 레벨의 태스크(자연어 추론 등), paraphrasing(의역), 토큰-레벨 태스크(개체명 인식, 질의응답 등) 등이 있다.

## 기존의 Pre-trained Language Representations

크게 다음과 같이 feature-based 와 fine-tuning 두 가지 종류가 있다.

- Feature-based (ELMo 등)
  - Task-specific 한 모델을 따로 설계한다.
  - 설계한 모델에서 pre-trained representation 을 하나의 feature 로 추가하여 학습시킨다.

- Fine-tuning (GPT 등)
  - 이는 위와 반대로 task-specific 한 모델을 따로 설계하지 않고,Task-specific 한 파라매터를 최소한으로 줄인 공통적인 모델을 사용한다.
  - 해당 모델로부터 task-specific 파라매터들을 해당 태스크에 맞게 fine-tuning 시켜서 학습시킨다. 이 때의 태스크를 downstream task 라고 부른다.

두 방식 모두 사용하는 목적 함수도 같고, 모델도 unidirectional (단방향) 언어 모델을 사용한다는 공통점이 있다.

## BERT의 Pre-trained Representations

이 논문은 위와 같은 기법들, 특히 fine-tuning 을 사용한 방식들은 pre-trained representation가 가진 힘을 제대로 발휘하지 못하게 제한하고 있다고 하고, 그 가장 큰 이유가 바로 언어 모델이 unidirectional 하기 때문이라고 하였다.

따라서 BERT는 fine-tuning 방식을 베이스로 하여 bidirectional (양방향) 모델을 구현한 것이 가장 큰 특징이다.

여기서 방향이란, 바로 문장의 중간에 어떤 단어와 나머지 다른 단어들을 참고할 때의 방향을 의미한다. 단방향의 경우, 예를 들어 GPT에서는 문장에서 해당 단어 이전의 단어들만 참고해서 attention을 수행하는 것이다. 반대로 해당 단어 이전의 단어들과, 이후에 나오는 단어들을 모두 참고하는 것이 바로 양방향 언어 모델이다.

BERT는 이 양방향을 Masked Language Model(MLM)을 통해 구현했다. MLM은 쉽게 말해서, 문장에서 임의의 토큰(단어)를 mask로 가리고, 그 단어가 무엇일 지 예측하는 방식이다.

또한, pre-training 에서 Next Sentence Prediction(NSP) 태스크를 포함시켜서 text-pair representation도 학습시킨다. NSP란, 말 그대로 다음 문장을 예측하는 학습을 말하고, text-pair representation 은 한 쌍의 두 문장을 입력으로 표현하는 것을 의미한다. 따라서 두 문장을 입력받아서, 해당 문장이 next sentence가 되는지, 즉 두 문장이 이어지는지를 구분하는 학습을 추가시켰다는 뜻이다.

이 text-pair 입력을 활용하는 분야 중 하나가 바로 자연어 추론 문제이다. 자연어 추론은 두 문장이 서로 어떤 관계에 있는지를 추론해 내는 분야다. 이러한 분야또한 BERT를 통해서 해결할 수 있게 설계된 것이다. 이런 면에서 정말 여러가지의 NLP 문제들을 BERT 하나로 풀어낼 수 있도록 하는 것에 초점을 두었다는 것을 알 수 있다.

# 2. BERT

BERT는 크게 다음 두 가지 단계로 진행된다.

1. **Pre-training**

   unlabled data 로부터 모델을 사전학습시키는 단계

2. **Fine-tuning**

   사전학습된 모델을 downstream task에서 사용되는 labeled data 로 파라매터를 fine-tuning 시키는 단계. 이 논문에서는 여러가지 종류의 downstream task에 대해 실험하였고, 동일한 pre-trained 모델로부터 각 downstream task에 대한 fine-tuning을 진행하였다.

이 진행 단계들에 대해서 설명하기 전에 먼저 아래 사항들에 대해 정리한다.

### 모델 구조

BERT의 모델은 기본적으로 기존 transformer 논문의 구조와 거의 동일하다.

L을 레이어의 개수, H는 히든 레이어의 크기, A를 self-attention 의 헤드 개수라고 할 때, BERT 논문에서 사용한 값들은 다음과 같다.

- BERT-base

  L=12, H=768, A=12

- BERT-large

  L=24, H=1024, A=16

BERT-base 모델은 GPT와 성능을 비교하기 위해 GPT에서 사용했던 동일한 크기로 모델을 생성했고, bidirectional self-attention을 적용했다는 점이 GPT와 다르다.

또한, 논문의 appendix에서 transformer에서는 ReLU를 사용했지만, BERT모델은 GELU를 activation 함수로 사용했다고 한다.

### 입력과 출력 representation

입력 representation을 설명하기 위한 요소는 다음과 같다.

- **sequence**
  - 이 논문에서 sequence가 하나의 입력 단위가 되는데, 이 값에는 그냥 하나의 문장이 들어올 수도 있고, 한 쌍의 문장이 합쳐져서 들어올 수도 있다.
  - 입력되는 문장에 언어적으로 말이 안되는 단어들을 나열한 값이 들어올 수도 있다.
- **token (토큰)**
  - 이 논문에서는 단어에 대한 토큰 임베딩으로 WordPiece[^1]를 사용한다.
  - 각 sequence의 첫 번째 토큰은 무조건 `[CLS]`로 시작하고, 문장 사이에는 `[SEP]`토큰을 삽입한다.
- **embedding (임베딩)**
  - 각 입력 임베딩에는 기본적으로 transformer 모델과 마찬가지로 토큰 임베딩과 positional 임베딩을 합친다.
  - 여러 문장을 합쳐서 sequence에 입력시키는 경우에는, 각 토큰마다 소속된 문장을 나타내는 학습된 임베딩도 추가한다. 이를 segment embedding 이라고 한다. 예를 들어서 첫 번째 문장은 0을 주고, 두 번째 문장은 1을 넣어서 구분하고, 하나의 문장만 사용할 경우 0으로 채울 수 있다.

이렇게 만들어진 입력 임베딩을 $E$로 표기한다.

BERT 모델에서 final hidden vector들이 바로 출력 representation이다. 각 final hidden vector들은 입력 토큰과 1:1로 다음과 같이 대응된다.

- 입력의 첫번째, 즉 `[CLS]`토큰에 대응하는 final hidden vector를 $C$라고 한다.
- 나머지 입력토큰들의 $i$번째에 해당하는 벡터는 $T_i$라고 한다.
- $C,T_i\in \mathbb{R}^H$이다. 즉, 각 hidden vector들은 동일하게 $H$의 차원을 가진다.

[^1]: Wordpiece 방식은 각 단어를 구성하는 성분을 "##"을 이용해서 쪼개서 토큰화시킨다. 예를 들어서, `embeddings`이라는 단어가 주어지면, 이를 `em`, `##bed`, `##ding`, `##s`의 4가지 토큰으로 쪼개지는 방식이다.

## 2.1. Pre-training

위에서도 언급되었듯이, 사전학습 단계는 MLM과 NSP 두 가지의 태스크가 있다.

### Masked Language Modeling (MLM)

직관적으로 생각해봐도, bidirectional 하게 참조하는 모델이 한쪽 방향으로 참조하는 모델보다 좋을 수밖에 없다. 하지만 BERT 이전의 논문들이 단방향 모델을 사용했던 이유는, 결국 의도한대로 양방향 모델이 동작하지 않았기 때문이다.

양방향을 참조했을 때 가장 큰 문제점은 바로 참조의 순환이다. 단방향은 무조건 한쪽 방향으로만 참조가 진행되기 때문에 순환문제가 생기지 않는다. 하지만 참조가 앞으로 갔다가 뒤로 갔다가를 반복하면, 결국 간접적으로 자기 자신을 참조하게 된다. 또한, 학습과정에 있어서 타겟까지 포함되어 학습되는 경우도 발생한다. 이럴 경우 학습이 정상적으로 이루어지기 힘들다.

MLM은 bidirectional representation을 학습시키기 위해서 입력 토큰들 중 일부를 masking 하여 가리고, 그 가려진 토큰이 어떤 것인지를 예측하는 과정이다.

BERT논문은 모든 실험에서 15%비율을 적용하여, 입력 토큰들을 각 sequence마다 랜덤으로 가리도록 설정하였다. 또한, autoencoder 같이 전체 sequence를 복구하는 문제와 다르게, 가려진 토큰만 찾아내는 방식을 적용하였다. 가려진 토큰은 마지막 hidden vector들이 softmax 레이어를 통해서 예측이 이루어지게 된다.

이렇게 MLM을 적용하는 방식의 단점은, pre-training 단계와 fine-tuning 단계가 이 `[MASK]`토큰때문에 토큰들이 어긋나게 된다는 것이다. `[MASK]`토큰은 pre-training 단계에만 존재하는 토큰이기 때문이다. 이 문제를 해결하기 위해, 논문에서는 무조건 `[MASK]`토큰을 적용하지 않고, 다음과 같은 방법을 적용하였다.

1. 80%의 확률로 `[MASK]` 토큰을 적용한다.
2. 10%의 확률로 마스킹 부분에 그냥 랜덤한 토큰을 적용한다.
   - 이는 즉, 다시 말해서 잘못 삽입된 단어를 찾아내고 교정하는 것을 학습하는 것이다.
3. 10%의 확률로 토큰을 가리지 않은 채로 해당 토큰을 예측하도록 한다.
   - 이는 사실상 대상이 어떤 토큰인지 구분할 수가 없으므로, 결국 정답을 찾을 수 없는 문제가 섞여있는 셈이 된다.
   - 이런 부분을 추가한 이유에 대해서 논문의 Appendix에서 좀 더 자세히 설명하고 있는데, 결론만 얘기하자면 결국 이렇게 추가했을 때 성능이 가장 좋았기 때문이다. 심지어 2번의 랜덤토큰보다도 학습에 유의미한 차이를 보여주었다.
   - 논문에서는 이러한 결과에 대한 이유를 따로 설명하고 있지는 않다. 하지만 이렇게 유의미한 결과를 내는 이유를 추측해보면, 결국 학습에서 overfitting을 방지하기 위한 역할인 것으로 생각된다.
     위에 언급한 것처럼 실제 fine-tuning 단계에서는 `[MASK]`토큰이 활용되지 않고, 정상적인 문장이 들어오기 때문이다. 따라서 `[MASK]`토큰이나, 랜덤 토큰만에 대해서 최적화가 되어버리면, 이후 단계에서는 오히려 정상적인 입력 문장을 처리하는데 방해가 될 수 있는 것이다.

### Next Sentence Prediction (NSP)

QA(Question Answering), NLI(Natural Language Inference) 등 많은 downstream task들이 두 문장사이의 관계를 학습시켜야 한다. 하지만 이러한 부분은 기존의 일반적인 언어 모델을 직접 적용할 수 없고, 해당 태스크마다 이러한 모델을 구현해야 했다.

BERT에서는 NSP 과정을 포함시키면서, 문장사이의 관계를 파악하는 모델도 같이 일반화시켰다. 위의 bidirectional을 적용시킨 MLM은 기존의 transformer보다 그냥 성능적인 향상 정도였다면, NSP는 단순히 성능뿐만 아니라, 이 학습방법을 적용시키면서 모델 자체의 역할을 확장시켰다고 볼 수 있다.

문장 관계를 학습시키기 위해서, 이 논문에서는 binarized NSP 태스크를 수행한다. Binarized NSP란, 학습에서 입력할 두 문장을 선택할 때, 50%는 바로 다음에 오는 문장을, 50%는 관련없는 문장을 선택해서 NSP에 사용하는 것을 의미한다.

NSP는 매우 간단한 작업이지만, QA와 NLI에 매우 효과적으로 성능 향상이 있었다.

### Pre-training에 사용하는 데이터

이 논문에서 사용한 데이터는 다음과 같다.

- BooksCorpus (800M words)
- English Wikipedia (2500M words)
  - 리스트, 테이블, 헤더 등을 제외한 텍스트들만 사용

여기서 중요한 점은, pre-training에서 문서레벨의 corpus를 사용하는 것이 문장레벨의 corpus(예를 들어 Billion Word Benchmark)를 사용하는 것보다 연속된 긴 문장 추출에 더 효과적이라는 것이다.

## 2.2. Fine-tuning

Transformer의 self-attention 구조 자체가 많은 downstream 태스크를 적용할 수 있기 때문에, Fine-tuning  과정은 매우 간단하다.

또한, 입력이 한 문장이든 두 문장이든 간단하게 처리할 수 있다. 기존에 두 문장의 입력 처리는 각각의 문장을 독립적으로 인코딩 하고, 두 문장 사이의 bidirectional cross attention을 수행했다. 하지만 BERT에서는 이를 한번에 처리할 수가 있다.

위와 같이 BERT는 두 문장의 입력을 하나의 sequence로 처리한다. 그 후에 self-attention을 수행하기 때문에, 이 self-attention 과정 안에 두 문장 사이의 bidirectional cross attention이 이미 포함되어있기 때문에 별도의 처리가 없어도 된다.

Fine-tuning은 각 태스크에서 task-specific한 입력과 출력 부분을 BERT모델과 연결시키고, 그 후에 모든 파라매터에 대해 fine-tuning을 진행한다.

입력에서 pre-training 단계의 문장 A와 문장 B는 Paraphrasing (해설), Entailment (의미 함축) 에서의 전제-가설, Question answering 에서의 질문-구절, Text classification 이나 sequence tagging에서의 degenerate 등의 분야에 대한 text pair와 유사하게 구성된다.

출력에서는 token representation 들은 토큰레벨의 태스크에 대한 output layer에 보내고, `[CLS]` representation은 entailment 나 sentiment analysis 와 같이 classification에 대한 output layer로 보낸다.

Fine-tuning 단계는 pre-training 단계에 비해서 계산 비용이 적게 발생한다. 대부분의 태스크들이 단일 TPU나 GPU로 몇 시간 이내에 결과를 얻을 수 있다. 구체적인 태스크에 대한 실험과 결과에 대해서 아래에서 다루도록 한다.

# 3. Downstream tasks

아래에서는 BERT에서 fine-tuning 을 수행하는 각 downstream 태스크들의 종류와 적용 방법을 중점으로 다루도록 한다. 원래 논문에서는 해당 결과를 중심으로 다루었지만, 결과가 좋다는 것은 이미 알고 있기 때문에 여기서는 그 결과까지 도달하는 과정을 파악하는 것에 더 중점을 두는 것이다.

## 3.1 GLUE (General Language Understanding Evaluation)

GLUE는 자연어의 의미를 이해하는 관점에서의 태스크에 대한 모델을 평가하는 벤치마크이다. GLUE에는 다음과 같이 9개의 태스크에 대한 데이터셋과 테스트 측정방법을 포함하고 있다.

### GLUE의 종류

- MNLI (Multi-Genre Natural Language Inference)

  여기서 NLI, 즉 자연어 추론이란 보통 entailment (의미 함축) 문제를 뜻한다. 한 쌍의 문장이 주어지면, 각 문장의 의미적인 부분에서의 포함관계를 파악한다.

  MNLI도 역시 entailment classification 태스크인데, Multi-Genre 라는 이름에서처럼 데이터셋이 매우 방대하고, 여러 소스의 데이터들이 섞여있다는 점이 특징이다. MNLI에서는 하나의 문장 쌍이 주어지면, 두 번째 문장이 첫번째 문장에 대해 3개의 label 중 어디에 해당하는지를 분류한다.

  - 종류 : classification

  - 입력 : sentence pair

  - 레이블 : entailment, contradiction, neutral
    - Entailment (의미 함축)

      두 번째 문장의 의미가 첫 번째 문장의 의미를 포함하고 있는 경우. 여기에 해당할 경우, 첫 번째 문장이 premise(전제)가 되고, 두 번째 문장이 hypothesis(가설)이 된다.

    - Contradiction (모순)

      두 번째 문장과 첫 번째 문장이 서로 반대되는 의미를 가지는 경우

    - Neutral (중립)

      두 번째 문장과 첫 번째 문장의 의미가 독립적인 경우

  - 평가 방법 : accuracy

- QQP (Quora Question Pairs)

  QQP는 Quora 라는 질문답변 사이트에서 올라오는 질문의 쌍에 대한 데이터에 대한 binary classification 태스크이다. 하나의 질문 쌍이 주어지면, 그 것이 같은 질문을 나타내는 지에 대한 여부를 분류하는 것이다.

  - 종류 : binary classification
  - 입력 : sentence pair
  - 레이블 : same, not same
  - 평가 방법 : accuracy

- QNLI (Question Natural Language Inference)

  QNLI 는 Stanford Question Answering Dataset(SQuAD)을 binary classification 태스크로 변환시킨 태스크이다. 원래의 SQuAD는 질문 문장-답변 문단의 쌍으로 구성되어, 해당 문단 중에서 정답 내용이 있는 문장을 찾는 문제이다.
  QNLI는 이를 질문 문장-답변 문장 쌍으로 변형시켜서, NLI라는 이름에서도 알 수 있듯이 entailment 문제로 만들었다. 답변 문장이 해당 질문에 대한 답변 내용을 포함하고 있는지에 대한 여부를 분류한다.

  - 종류 : binary classification
  - 입력 : sentence pair
  - 레이블 : entail, not entail
  - 평가 방법 : accuracy

- SST-2 (Stanford Sentiment Treebank - binary)

  SST는 영화 리뷰에서 단일 문장의 감정상태를 예측하는 데이터셋이다.

  여기서 Treebank란 어떤 구문에 다는 annotation, 주석을 트리 구조로 표현한 것을 의미한다. 여기서 주석이란, 일종의 태그라고 생각하면 되는데, 결국에 그 문장에 대한 어떤 종류(혹은 설명)을 적어놓은 개념이라고 이해하면 된다. 즉 다시 말해서, SST는 리뷰 문장이 input이 되고, 그 문장의 감정상태에 대한 주석이 바로 label이 되는 데이터셋인 것이다.

  SST-2에서 뒤의 2의 의미는 label의 개수가 2개라는 뜻으로, 즉 binary classification으로 분류하는 문제이고, 5개의 label을 사용하는 SST-5가 있다.

  - 종류 : binary classification
  - 입력 : single sentence
  - 레이블 : positive, negative
  - 평가 방법 : accuracy

- CoLA (Corpus of Linguistic Acceptability)

  CoLA는 어떤 문장이 주어지면, 그 문장에 대해 언어학적으로 "acceptable", 즉 수용 가능한 문장인지에 대한 여부를 예측하는 문제이다.

  - 종류 : binary classification
  - 입력 : single sentence
  - 레이블 : acceptable, not acceptable
  - 평가 방법 : accuracy

- STS-B (Semantic Textual Similarity benchmark)

  STS란, 문장 쌍이 주어졌을 때, 두 문장이 얼마나 유사한지에 대한 정도를 예측하는 문제이다. 위의 문제들과는 다르게, 0~5 사이[^2]의 유사도를 구하는 regression 문제이다. STS-B는 SemEval이라는 워크샵의 STS 태스크에서 2012~2017년 기간에 사용된 영어 데이터셋으로, 뉴스 헤드라인과 다른 소스들을 포함하고 있다.

  - 종류 : regression
  - 입력 : sentence pair
  - 레이블 : 0 ~ 5
  - 평가 방법 : Pearson correlation, Spearman correlation (논문에서는 Spearman correlation을 사용함)

- MRPC (Microsoft Research Paraphrase Corpus)

  MRPC는 온라인 뉴스에서 자동으로 추출된 문장 쌍이 의미상 같은지에 대한 여부를 예측하는 문제이다.

  - 종류 : binary classification
  - 입력 : sentence pair
  - 레이블 : same, not same
  - 평가 방법 : accuracy

- RTE (Recognizing Textual Entailment)

  RTE는 MNLI 와 유사한 entailment classification 문제이다. 하지만 MNLI보다 더 적은 트레이닝 데이터로 예측을 수행해야 하며, binary classification이라는 차이점이 있다. QNLI와 같이 binary classification 문제이다.

  - 종류 : binary classification
  - 입력 : sentence pair
  - 레이블 : entail / not entail
  - 평가 방법 : accuracy

- ~~WNLI (Winograd Natural Language Inference)~~

  WNLI 역시 NLI데이터셋으로, entailment에 대한 문제이다. 하지만 GLUE에서 이 데이터셋에 이슈가 있다고 밝혀, BERT논문에서는 이 태스크를 제외시키고 측정을 진행했다.

[^2]: BERT 논문에서는 1~5 사이라고 되어있으나, 다른 자료들은 대부분 0~5라고 설명한다.

### GLUE 모델 구성

GLUE에서 fine-tuning 모델을 구축할 때, final hidden vector에서 첫 번째 입력 토큰 `[CLS]`과 대응하는 representation, 즉 벡터 $C$의 값들을 입력으로 사용한다. 이 입력이 classification layer (혹은 regression layer) $W\in \mathbb{R}^{K\times H}$로 들어가서 최종적인 결과를 얻게 된다. 여기서 $K$는 label의 개수를 뜻하며, regression의 경우 1개의 실수 형태의 최종 output을 가지게 된다.

## 3.2 SQuAD (Stanford Question Answering Dataset)

SQuAD는 여러 소스들에서 모인 질문/답변 쌍 데이터이다. 하나의 질문과, 그에 대한 답변 문단이 들어있고, 답변 문단 중에서 질문에 대한 답이 되는 구문을 찾는 것이 목적이다.

BERT에서는 해당 질문과, 전체 답변 문단을 전부 하나의 input으로 처리하고, `[SEP]`로 구분한다.

$S,E$가 각각 시작 벡터와 끝 벡터라고 하면, 답변 문단 중에서 $i$번째 단어가 답변 구문의 시작일 확률은 다음과 같이 softmax를 통해서 계산할 수 있다.

$$
P_i=softmax(S\cdot T_i)=\frac{e^{S\cdot T_i}}{\sum_je^{S\cdot T_j}}
$$

여기서 후보 문구가 $i$부터 $j$토큰이라고 하면, 점수는 다음과 같이 계산한다.

$$
\hat{s_{i,j}}=max_{j\ge i}(S\cdot T_i+E\cdot T_j)
$$

$\hat{s_{i,j}}$를 가장 크게 만드는 $i,j$가 바로 정답 구문이 되는 것이다.

이 논문에서는 이 $i,j$에 대해서 log-likelihood, 즉 cross entropy를 통해서 각각의 label을 classification하는것과 같이 학습을 시켰다.

### SQuAD v1.1 vs v2.0

SQuAD v2.0 은 SQuAD v1.1에서 해당 문단 내에 답변이 없을 수 있는 경우를 추가한 태스크다. 이 논문에서는 이에 대해 아주 간단한 방법으로 접근했다. 바로 답이 없는 경우를 그냥 시작과 끝이 `[CLS]`토큰, 즉 맨 처음인 것으로 처리해서 학습을 시키는 것이다.

이렇게 진행할 경우에, 답이 없는 경우에 대한 예측이 너무 적을 수 있다. 시작토큰과 끝 토큰이 모두 `[CLS]`를 만족하는 경우에만 답이 없다고 판단하기 때문이다. 따라서 이 논문에서는 threshold를 적용해서 다음과 같이 예측을 진행한다.

$$
s_{null}=S\cdot C+E\cdot C\\ \hat{s_{i,j}}>s_{null}+\tau
$$

논문에서는 이 $\tau$를 실험을 통해 F1 스코어[^1]를 가장 높게 만드는 값을 구해서 설정했다.

[^1]: $2/(precision^{-1}+recall^{-1})$

## 3.3 SWAG (Situations With Adversarial Generations)

SWAG는 grounded common-sense, 즉 기초적인 상식을 추론하기 위한 데이터셋으로, 어떤 문장이 주어졌을 때 4개의 선택지 중에서 가장 그럴듯하게 이어지는 문장을 고르는 것이 목적이다. 이 4개중 하나의 선택지를 고르는 태스크를 구현하기 위해 BERT에서는 다음과 같은 방식을 사용했다.

- 각각의 선택지마다 <주어진 문장-선택지 문장>의 문장 쌍을 구성하여, 하나의 문장에서 4개의 input sequence가 생성될 수 있도록 한다.
  - 즉, 하나의 문제가 4개의 batch input으로 바뀐다고 보면 된다.
- 4개의 input sequence를 BERT를 통해 수행해서 나온 각각의 output representation을 구한다.
- 이 4개의 output에서 `[CLS]`토큰에 해당하는 representation $C$ 와 task-specific 파라매터 $V$를 스칼라곱($\cdot$)한다.
- $C\cdot V$ 나온 결과가 각 선택지에 대한 점수가 되고, 이에 대해서 softmax를 통해서 정답 문장에 대한 classification 문제를 수행한다.

# 4. Ablation Studies

Ablation이란, 일정 부분을 제거한다는 뜻이다. 따라서 ablation study는, 말 그대로 일정 부분을 제거하면서 그 결과를 통해 해당 부분이 어떤 역할을 하는 지를 파악하는 실험이다. 딥러닝에서는 어떤 모델을 구성한 이유에 대해서 수식적으로 증명하거나 이해시킬 수 있는 방법이 없다. 따라서 이러한 실험을 통해, 모델에 대한 의문점을 어느정도 해소하려고 하는 것이다.

BERT논문에서는, 모델에서 크게 기존 transformer 모델에 MLM과 NSP라는 개념을 도입하였고, 성능 향상을 이루었다. 하지만 정말로 이 bidirectional 개념과, NSP 작업이 정말 효과적으로 작용했는지는 그냥 추측일 뿐이다. 왠지 양방향으로 attention을 하니, 단방향보다 효과적일 것 같고, NSP를 통해 문장사이의 관계를 학습해서 태스크 수행에 좋을 것 같다는 생각일 뿐이고, 이러한 생각은 틀릴 수도 있는 것이다. 예를 들어서, bidirectional은 아무 효과가 없는데 NSP때문에 결과가 좋아진 것일 수도 있다. 

그래서 논문에서는 이 ablation study를 통해 두 개념의 각각의 효과에 대해 증명하고자 했다. 물론 항상 ablation study로 모든 것을 설명할 수 있는 것은 아니다. 예를 들어서, 두 방식을 함께 적용했을 때만 시너지가 나는 방식이 있다면, 이러한 것은 ablation study를 통해서 설명하기 어렵다. 하지만 이 논문에서는 각각의 방식이 독립적으로 효과적이었고, 따라서 ablation study를 통해 이를 증명할 수 있었다.

실험 방식은, 기존의 transformer 모델에서 각각의 가능한 조합들을 구성해서, 위의 태스크들에 대해서 결과물들을 비교했다. 수치 자체를 설명하는 것은 큰 의미가 없기 때문에 각 결과에 대한 해석에 대해 중점을 두어 설명하도록 한다. 아래 내용들은 논문에서 ablation study 부분과, appendix 내용들을 모두 합쳐서 중요한 내용들을 요약하였다.

## 4.1 NSP vs No NSP

1. **vs No NSP 모델** (즉, MLM만 적용한 BERT 모델)
    : BERT 모델이 결과가 좋음
2. **vs No NSP + LTR(Left To Right) 모델** (즉, MLM, NSP 둘 다 적용하지 않은 BERT모델)
    : BERT 모델이 결과가 좋음 + 위 No NSP 모델보다 결과가 더 나쁨

따라서 MLM과 상관없이 NSP 학습이 효율적이라는 것을 증명할 수 있다.

## 4.2 MLM

1. No NSP의 경우에는 위에서와 같이 MLM 적용한 모델이 더 효과적이었다.
2. **vs LTR BERT** (NSP 만 적용한 BERT)
   : 초기 수렴은 LTR BERT가 더 좋았지만, pre-training steps가 증가할수록 MLM이 LTR보다 더 좋은 결과를 얻음.
   - 추가로 SQuAD 태스크에 대해 다음과 같은 실험을 진행했다.
     - SQuAD 태스크의 특성상, 뒤의 문장에 대해 참조하는 부분이 없으면 너무 불리하다.
     - 따라서 논문에서는 LTR에 BiLSTM을 추가로 적용하여 실험을 진행했다.
     - SQuAD 결과는 비교적 좋아졌지만 BERT모델보다 나쁜 결과를 얻었다. GLUE 결과는 오히려 떨어졌다.
### MLM의 각 마스킹 토큰 적용

MLM에서 가려진 부분에 대해 `[MASK]` 토큰을 적용하거나, 원래 토큰을 주거나, 랜덤 토큰을 주는 3가지 방식을 각각 80%:10%:10%의 비율로 수행했다. 위에서도 언급했다시피, 실험적으로 이러한 방식이 가장 결과가 좋았다. 다른 부분에 대해서 주목할만한 점은, 바로 랜덤토큰의 효과에 대한 부분이다. 그냥 `[MASK]`토큰만 100% 주는 경우와 비교할 때, 랜덤 토큰과 원래 토큰의 적용에 대해 다음과 같은 결과가 나왔다.

- 원래 토큰을 주지 않고, 랜덤 토큰만 20% 줄 경우, 오히려 `[MASK]`토큰만을 적용할 경우보다 나쁜 결과가 나왔다.
- 반대로, 랜덤 토큰을 주지 않고, 20%를 원래 토큰을 주는 경우에는 `[MASK] `토큰만을 적용할 경우보다 좋은 결과가 나왔다.
  심지어 태스크에 따라 이 방식이 80%:10%:10% 적용시의 결과보다 더 좋은 결과가 나오기도 했다.

이렇게까지만 보면, 랜덤 토큰이 필요없다는 생각이 들 수도 있다. 하지만 결과적으로, 원래 토큰과 랜덤 토큰을 반반 줄 경우에는 오히려 원래 토큰만 주는 경우보다 성능이 향상되었다. 따라서 랜덤 토큰은 어느정도 원래 토큰이 있어야 학습효과를 발생시킨다고 말할 수 있다.

또한, 이 결과 차이가 원래 토큰을 20%주는 경우와 큰 차이가 나지 않는다. 즉, 이 랜덤토큰은 향후 모델의 개선이나 다른 태스크들에서는 얼마든지 사라질 가능성이 있다고 생각한다.

## 4.3 기타 실험

논문에서는 기타 수치들에 대해서도 많은 실험을 진행했다. 하지만 BERT에 대한 이해의 관점에서는 이러한 부분은 크게 중요하지 않기 때문에, 다음과 같이 간략하게 요약만 하도록 한다.

- 모델의 크기가 커질수록 좋은 결과를 얻는다.
  - 일반적으로 데이터가 충분하지 않다면, 모델의 크기가 클 때 overfitting이 발생할 수도 있다.
  - 하지만, BERT는 태스크의 데이터가 적은 경우에도 모델의 크기가 클 때 좋은 결과가 발생했다. 이는 pre-training 단계에서 충분히 많은 데이터를 주기 때문에, 이 데이터들이 큰 모델에 대해서 좋은 representation을 만들어주기 때문이라고 할 수 있을 것으로 보인다.
- Feature-based 방식을 적용해도 효과적이었고, Fine-tuning 방식이 약간 더 좋다.

# 결론

딥러닝에서 가장 활발히 연구되는 분야가 바로 vision과 NLP이다. Vision 분야는 이미 CNN을 기반으로 하여 ResNet의 등장으로 빠른 발전을 이루어냈고, 많은 방식들이 정립됐었다. 하지만 RNN에 기반했던 NLP는 인간과 성능을 비교하기 시작한 CNN에 비해서 비교적으로 발전이 더뎠다.

Transformer의 등장으로 NLP의 모델이 어느정도 정립되었고, BERT논문으로 사전학습을 통한 공통된 representation이 여러 NLP 문제들에 대해 매우 효과적이라는 것을 입증했다. 이후 BERT에 기반한 많은 후속 연구들이 등장하고, NLP 분야가 정말 빠른 속도로 발전해 나가고 있다.

BERT논문의 내용은 사실 매우 간단하다. 거의 transformer의 모델을 그대로 사용하고, 학습 방법에 대해서만 정리한 느낌이다. 하지만 이 간단한 논문으로 전이학습이 보편화되고, 많은 BERT 관련 pre-trained 모델 데이터를 활용해서 적은 데이터를 가지고도 다양한 태스크에 도전할 수 있게 되었다.

